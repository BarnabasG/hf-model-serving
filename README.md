# model-serving-monitoring
End-to-end LLM inference service: FastAPI + HuggingFace, containerized for Kubernetes with autoscaling, telemetry, and latency monitoring
