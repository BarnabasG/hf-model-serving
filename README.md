# hf-model-serving
End-to-end LLM inference service: FastAPI + HuggingFace, containerized for Kubernetes with autoscaling, telemetry, and latency monitoring
